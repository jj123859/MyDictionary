{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41c2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "from konlpy.tag import Twitter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tag import untag\n",
    "from nltk import Text\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import FreqDist\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": \"C:/Users/kimEn/Desktop\", #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "})\n",
    "driver = webdriver.Chrome('chromedriver.exe', options=options)\n",
    "\n",
    "driver.get('https://paperswithcode.com/sota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bf342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#맨처음 목록 목차 분류작업\n",
    "\n",
    "class_2b_url=[]  #big 분류페이지로\n",
    "class0=[] #class_big와 길이확인**\n",
    "\n",
    "#browse sota대분류 class0 -  중분류 class1  소분류 -pdf설명페이지-pdf다운 길이확인ㅇ\n",
    "#class0=맨처음페이지 대분류 리스트, class0_url=대분류페이지 이동url\n",
    "list1=driver.find_elements(By.CLASS_NAME,'col-md-12 > h4 > a')\n",
    "for i in list1:\n",
    "    class0.append(i.text)\n",
    "    class_2b_url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25f9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#처음목차와 다음 목차 사전 저장& 넘어갈 url수집\n",
    "dic_bAndm={}\n",
    "dic_mAnds={}\n",
    "class_big=[]\n",
    "class1=[]  #class m과 길이비교**\n",
    "class_b2m_url=[]\n",
    "class_small=[]\n",
    "class_m2s_url=[]\n",
    "class_small_dic=[]\n",
    "tmp=[]\n",
    "mid_url=[]\n",
    "\n",
    "#대분류url로 이동\n",
    "for i in range(len(class_2b_url)):\n",
    "    driver.get(class_2b_url[i]) \n",
    "    \n",
    "#class1=computer vision하위 518개 \n",
    "#class1=대분류 내 중분류 리스트, class1_url=see all 클릭시 넘어가는 url / none일경우 소분류로 넘어가기 추가**** \n",
    "    list1=driver.find_elements(By.CLASS_NAME,'infinite-container.featured-task > div.row > div.col-md-12 > h2')\n",
    "    list4=driver.find_elements(By.CLASS_NAME, 'col-lg-12 > h1') #dic1&2(대-중분류) key\n",
    "    list3=driver.find_elements(By.CLASS_NAME,'col-xl-8.card-col.card-col-title > h1') #소분류text\n",
    "    \n",
    "    for i in list4:\n",
    "        class_big.append(i.text)    \n",
    "        for j in list1:             \n",
    "            tmp.append(j.text)\n",
    "            dic_bAndm[i.text]=tmp  #대-중dictionary(list형식)\n",
    "        tmp=[]\n",
    "        \n",
    "#         for k in list3:\n",
    "#             class_small_dic.append(k.text)\n",
    "#             dic_mAnds[class1]=class_small_dic   #중-소분류 사전 수정\n",
    "    \n",
    "    #mid_url = 중분류에서 바로 소분류로 넘어가는 url\n",
    "    mid_url_elem=driver.find_elements(By.CLASS_NAME,'card > a')\n",
    "    for i in mid_url_elem:\n",
    "        if i not in mid_url:\n",
    "            mid_url.append(i.get_attribute('href'))\n",
    "    \n",
    "    list2 = driver.find_elements(By.CLASS_NAME,'sota-all-tasks > a')\n",
    "    for i in list2:          \n",
    "            class_b2m_url.append(i.get_attribute('href'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d8b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#중분류 모두보기 있는 경우 앞과 겹치지 않게 url 수집 mid_url에 저장\n",
    "\n",
    "for i in range(len(class_b2m_url)):\n",
    "    driver.get(class_b2m_url[i])\n",
    "    list4=driver.find_elements(By.CLASS_NAME,'card-title')\n",
    "    list5=driver.find_elements(By.CLASS_NAME,'card > a')\n",
    "    \n",
    "#      #소분류\n",
    "#     for i in list4:\n",
    "#         if i not in list4:\n",
    "#             class_small.append(i.text)\n",
    "     \n",
    "    #중분류 모두보기에서 소분류 url 수집\n",
    "    for i in list5:\n",
    "        if i not in mid_url:\n",
    "            mid_url.append(i.get_attribute('href'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e56ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#목록 df저장/csv저장-없는 것만 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2440e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문 제목, 상세 url 수집\n",
    "# classname>css section\n",
    "    \n",
    "title = []\n",
    "url = []\n",
    "pdf=[]\n",
    "\n",
    "#mid_url(소분류 url) 반복하면서 title, url 리스트 생성\n",
    "for i in mid_url:\n",
    "    driver.get(f'{i}')\n",
    "\n",
    "#title, url\n",
    "    Tlist=driver.find_elements(By.CLASS_NAME,'col-lg-9.item-content > h1 > a')\n",
    "    for i in Tlist:\n",
    "        title.append(i.text)\n",
    "        url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e8367b",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: cannot determine loading status\nfrom disconnected: received Inspector.detached event\n  (Session info: chrome=99.0.4844.84)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00639943+2595139]\n\tOrdinal0 [0x005CC9F1+2148849]\n\tOrdinal0 [0x004C4528+1066280]\n\tOrdinal0 [0x004B69BE+1010110]\n\tOrdinal0 [0x004B65AF+1009071]\n\tOrdinal0 [0x004B5BC6+1006534]\n\tOrdinal0 [0x004B4AD0+1002192]\n\tOrdinal0 [0x004B50D8+1003736]\n\tOrdinal0 [0x004C0AAC+1051308]\n\tOrdinal0 [0x004B63BD+1008573]\n\tOrdinal0 [0x004B6F7C+1011580]\n\tOrdinal0 [0x004B65CA+1009098]\n\tOrdinal0 [0x004B5BC6+1006534]\n\tOrdinal0 [0x004B4AD0+1002192]\n\tOrdinal0 [0x004B4FAD+1003437]\n\tOrdinal0 [0x004C5C4A+1072202]\n\tOrdinal0 [0x0051C19D+1425821]\n\tOrdinal0 [0x0050B9EC+1358316]\n\tOrdinal0 [0x0051BAF2+1424114]\n\tOrdinal0 [0x0050B806+1357830]\n\tOrdinal0 [0x004E6086+1204358]\n\tOrdinal0 [0x004E6F96+1208214]\n\tGetHandleVerifier [0x007DB232+1658114]\n\tGetHandleVerifier [0x0089312C+2411516]\n\tGetHandleVerifier [0x006CF261+560433]\n\tGetHandleVerifier [0x006CE366+556598]\n\tOrdinal0 [0x005D286B+2173035]\n\tOrdinal0 [0x005D75F8+2192888]\n\tOrdinal0 [0x005D76E5+2193125]\n\tOrdinal0 [0x005E11FC+2232828]\n\tBaseThreadInitThunk [0x7542FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x771D7A7E+286]\n\tRtlGetAppContainerNamedObjectPath [0x771D7A4E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9304/2159306155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpdf_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'badge.badge-light'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m--> 437\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    427\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: unknown error: cannot determine loading status\nfrom disconnected: received Inspector.detached event\n  (Session info: chrome=99.0.4844.84)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00639943+2595139]\n\tOrdinal0 [0x005CC9F1+2148849]\n\tOrdinal0 [0x004C4528+1066280]\n\tOrdinal0 [0x004B69BE+1010110]\n\tOrdinal0 [0x004B65AF+1009071]\n\tOrdinal0 [0x004B5BC6+1006534]\n\tOrdinal0 [0x004B4AD0+1002192]\n\tOrdinal0 [0x004B50D8+1003736]\n\tOrdinal0 [0x004C0AAC+1051308]\n\tOrdinal0 [0x004B63BD+1008573]\n\tOrdinal0 [0x004B6F7C+1011580]\n\tOrdinal0 [0x004B65CA+1009098]\n\tOrdinal0 [0x004B5BC6+1006534]\n\tOrdinal0 [0x004B4AD0+1002192]\n\tOrdinal0 [0x004B4FAD+1003437]\n\tOrdinal0 [0x004C5C4A+1072202]\n\tOrdinal0 [0x0051C19D+1425821]\n\tOrdinal0 [0x0050B9EC+1358316]\n\tOrdinal0 [0x0051BAF2+1424114]\n\tOrdinal0 [0x0050B806+1357830]\n\tOrdinal0 [0x004E6086+1204358]\n\tOrdinal0 [0x004E6F96+1208214]\n\tGetHandleVerifier [0x007DB232+1658114]\n\tGetHandleVerifier [0x0089312C+2411516]\n\tGetHandleVerifier [0x006CF261+560433]\n\tGetHandleVerifier [0x006CE366+556598]\n\tOrdinal0 [0x005D286B+2173035]\n\tOrdinal0 [0x005D75F8+2192888]\n\tOrdinal0 [0x005D76E5+2193125]\n\tOrdinal0 [0x005E11FC+2232828]\n\tBaseThreadInitThunk [0x7542FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x771D7A7E+286]\n\tRtlGetAppContainerNamedObjectPath [0x771D7A4E+238]\n"
     ]
    }
   ],
   "source": [
    "#pdf_url 수집\n",
    "for i in range(len(url)):\n",
    "    \n",
    "    driver.get(url[i])\n",
    "    pdf_url=driver.find_elements(By.CLASS_NAME,'badge.badge-light')[0].get_attribute('href')\n",
    "    \n",
    "    if pdf_url not in pdf:\n",
    "        pdf.append(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['title','url','pdf'])\n",
    "\n",
    "df['title']=title\n",
    "df['url']=url\n",
    "df['pdf']=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#내용저장 csv\n",
    "df.to_csv(\"theory.csv\",header=True, index=False)\n",
    "df_theory=pd.read_csv('theory.csv')\n",
    "df_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f80373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os로 파일 다운로드\n",
    "pdf_url = df['pdf'].values.tolist()\n",
    "pdf_file = [i.split('/')[-1] for i in df['pdf'].values if 'arxiv' in i]\n",
    "\n",
    "print(f'전체 파일 수: {len(pdf_url)}')\n",
    "\n",
    "#enumerate -> index / for n, ( , ) = n=index, ( , )=tuple\n",
    "#start /b =>os background실행\n",
    "for n, (url, file) in enumerate(zip(pdf_url, pdf_file)):\n",
    "    print(n)\n",
    "    download_path = f'C:/Users/kimEn/Downloads/pdf_download'\n",
    "    os.system(f'curl \"{url}\" --output \"{download_path}/{file}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708638e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf text변환\n",
    "def pdf_to_txt(pdf_file):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "   \n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(pdf_file, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    \n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,\n",
    "                                 caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    \n",
    "    text = retstr.getvalue()\n",
    "    \n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    pdf_txt=[]\n",
    "    pdf_txt.append(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#오류제외 파일변환\n",
    "\n",
    "for i in tqdm(pdf_file):    \n",
    "    try:\n",
    "        text = pdf_to_txt(f'{download_path}/{i}')\n",
    "        txt_file_name = '.'.join(i.split('.')[:-1])\n",
    "        with open(f'{download_path}/txt/{txt_file_name}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "    except Exception as e:\n",
    "        print(e, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#어간추출\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_wordnet_pos(tagged_pos):\n",
    "    for pos in ['V', 'N', 'J', 'R']:\n",
    "        if tagged_pos.startswith(pos):\n",
    "            return pos.lower() if pos != 'J' else 'a'\n",
    "    return None\n",
    "\n",
    "\n",
    "# 어간 추출\n",
    "def get_stemmer(stemmer):\n",
    "    \"choose stemmer in 'porter' 'lancaster' 'snowball'\"\n",
    "    if stemmer == 'porter':\n",
    "        stemmer = PorterStemmer()\n",
    "    elif stemmer == 'lancaster':\n",
    "        stemmer = LancasterStemmer()\n",
    "    elif stemmer == 'snowball':\n",
    "        stemmer = SnowballStemmer('english')\n",
    "    return stemmer\n",
    "\n",
    "\n",
    "re_tokenizer = RegexpTokenizer('[a-z]{2,}')\n",
    "stemmer = get_stemmer('snowball')\n",
    "lemm = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "word_token_list = []\n",
    "for txt_file in tqdm(glob('./Downloads/pdf_download/txt/*.txt')):\n",
    "    with open(txt_file, encoding='utf-8') as f:\n",
    "        txt = f.read().lower().replace('-\\n', '')\n",
    "        \n",
    "    word_tokens = re_tokenizer.tokenize(txt)\n",
    "    word_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "    # stemed_txt = [stemmer.stem(w) for w in word_tokens]\n",
    "    \n",
    "    word_tokens = pos_tag(word_tokens)\n",
    "    word_tokens = [(w, get_wordnet_pos(tag)) for w, tag in word_tokens if get_wordnet_pos(tag) != None]\n",
    "    word_tokens = [lemm.lemmatize(word, pos=tag) for word, tag in word_tokens]\n",
    "    \n",
    "    word_token_list.extend(word_tokens)\n",
    "    \n",
    "FreqDist(word_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef70791",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Downloads/pdf_download/txt/1306.5532v2.txt', encoding='utf-8') as f:\n",
    "    txt = f.read().lower()\n",
    "\n",
    "# 토크나이저\n",
    "re_tokenizer = RegexpTokenizer('[a-zA-Z]{2,}')\n",
    "\n",
    "# 토큰화\n",
    "word_tokens = re_tokenizer.tokenize(txt)\n",
    "\n",
    "# 불용어(stopwords) 제거\n",
    "stop_words = stopwords.words('english')\n",
    "word_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# word_tokens = [stemmer.stem(w) for w in word_tokens]\n",
    "\n",
    "word_tokens = pos_tag(word_tokens)\n",
    "word_tokens = [(w, get_wordnet_pos(tag)) for w, tag in word_tokens if get_wordnet_pos(tag) != None]\n",
    "word_tokens = [lemm.lemmatize(word, pos=tag) for word, tag in word_tokens]\n",
    "\n",
    "FreqDist(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4642ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_txt=open([f'{download_path}/txt/{i}.txt' for i in txt_file_name],'r+',encoding=\"UTF-8\").read()\n",
    "word_tokenize(open_txt)\n",
    "\n",
    "retokenize=RegexpTokenizer('[\\w]+')\n",
    "retoken_txt=retokenize.tokenize(open_txt)\n",
    "\n",
    "retoken_txt\n",
    "\n",
    "#어간추출\n",
    "st1=PorterStemmer()\n",
    "st2=LancasterStemmer()\n",
    "\n",
    "print(\"Porter Stemmer:\", [st1.stem(w) for w in retoken_txt])\n",
    "print(\"Lancaster Stemmr\",[st2.stem(w) for w in retoken_txt])\n",
    "\n",
    "#원형복원\n",
    "lm=WordNetLemmatizer()\n",
    "[lm.lemmatize(w,pos=\"v\") for w in retoken_txt]\n",
    "\n",
    "#품사 부착\n",
    "sentence='This is for me the most important part of this thesis, as it would not be possible to do this alone.'\n",
    "tagged_list=pos_tag(word_tokenize(sentence))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9acda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#태그 튜플 제거\n",
    "untag(tagged_list)\n",
    "\n",
    "#같은 토큰, 다른 품사 구분\n",
    "def tokenizer(doc):\n",
    "    return [\"/\".join(p) for p in tagged_list]\n",
    "\n",
    "tokenizer(sentence)\n",
    "\n",
    "text=Text(retokenize.tokenize(open_txt))\n",
    "text.similar(\"alone\") #비슷한문맥의 비슷한단어\n",
    "text.common_contexts(['alone','context']) #두 단어 공통 문맥\n",
    "\n",
    "fd=text.vocab()\n",
    "type(fd)\n",
    "\n",
    "stopwords=['is','are']\n",
    "txt_tokens=pos_tag(retokenize.tokenize(open_txt))\n",
    "names_list=[t[0] for t in txt_tokens if t[1]==\"NNP\" and t[0] not in stopwords and len(t[0])!=1]\n",
    "fd_names=FreqDist(names_list)\n",
    "0\n",
    "fd_names.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e295994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도수 높은 단어들어간 문장 추출1\n",
    "import string\n",
    "\n",
    "f=open('./Downloads/pdf_download/txt/1306.5532v2.txt', encoding='utf-8')\n",
    "# replace_space(f)\n",
    "x=f.read()\n",
    "sentence=x.split('.')\n",
    "terminology=[]\n",
    "\n",
    "keywords=FreqDist(word_token_list)\n",
    "\n",
    "for k in keywords:#3번\n",
    "    for s in sentence:\n",
    "        if s.find(k)>0 and ((s.find('is')>0 and t[0] for t in tagged_list if t[1].startswith('V')) or (s.find('are')>0):  #word-token에서 verb만 일치하도록 변경\n",
    "            terminology.append(s)\n",
    "            print(s)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도수 높은 단어들어간 문장 추출2 사전저장\n",
    "import string\n",
    "\n",
    "keywords=FreqDist(word_tokens)\n",
    "terminology=[]\n",
    "\n",
    "for i in txt_file_name:\n",
    "    f=open(f'{download_path}/txt/{txt_file_name}.txt', 'r', encoding='utf-8')\n",
    "    x=f.read()\n",
    "    sentence=x.split('.')\n",
    "\n",
    "    for k in keywords:#3번\n",
    "        for s in sentence:\n",
    "            if s.find(k[0])>0 and s.find('is'):\n",
    "                terminology.append(s)\n",
    "                print(s)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74153b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#구글번역기로 번역\n",
    "from googletrans import Translator\n",
    "keywords=['3d','classification','recognition','predict']\n",
    "meaning={'3d':'3d explation','classification':'classification explation','recognition':'recognition explation','predict':'prediction explation'}\n",
    "Ttranslated=[]\n",
    "for i in meaning.values():\n",
    "    translator=Translator()\n",
    "    result=translator.translate(i,src='en', dest='ko')\n",
    "    Ttranslated.append(result.text)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a920886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(단어-분류)-(의미 번역-원문-출처) -> csv 저장\n",
    "import pandas as pd\n",
    "df2=pd.DataFrame(columns=['단어','설명','원문','출처'])\n",
    "\n",
    "df2['단어']=meaning.keys()\n",
    "df2['원문']=meaning.values()\n",
    "df2['설명']=Ttranslated\n",
    "\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
