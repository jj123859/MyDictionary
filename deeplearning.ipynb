{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27951544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "#from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import normalize\n",
    "import math\n",
    "#from konlpy.tag import Twitter\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tag import untag\n",
    "from nltk import Text\n",
    "#from matplotlib import pyplot as plt\n",
    "from nltk import FreqDist\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import collections\n",
    "import os.path\n",
    "from googletrans import Translator\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def first_collect_url():\n",
    "    \n",
    "    \"\"\"다음 페이지로 이동하는 경로 수집\"\"\"\n",
    "    \n",
    "    first_to_second_url_list = [] \n",
    "    cla1_cla2_dict = collections.defaultdict(list)\n",
    "    cla1_seeall_url_dict = collections.defaultdict(list)\n",
    "    third_url_list = []\n",
    "    see_all_url = []\n",
    "    \n",
    "    driver.get('https://paperswithcode.com/sota')\n",
    "    fs_url = driver.find_elements(By.CLASS_NAME,'col-md-12 > h4 > a')\n",
    "    \n",
    "    for url in fs_url:\n",
    "        first_to_second_url_list.append(url.get_attribute('href'))\n",
    "\n",
    "    for url in first_to_second_url_list:\n",
    "        driver.get(url)\n",
    "\n",
    "        cla1_elem = driver.find_elements(By.CLASS_NAME, 'col-lg-12 > h1')\n",
    "        cla2_elem = driver.find_elements(By.CLASS_NAME, 'col-xl-8.card-col.card-col-title > h1')\n",
    "        seeall_url_elem = driver.find_elements(By.CLASS_NAME, 'sota-all-tasks > a')\n",
    "        third_url_elem = driver.find_elements(By.CLASS_NAME,'card > a')\n",
    "\n",
    "        for cla1 in cla1_elem:\n",
    "            for cla2 in cla2_elem:\n",
    "                cla1_cla2_dict[cla1.text].append(cla2.text)\n",
    "            for seeall_url in seeall_url_elem:\n",
    "                cla1_seeall_url_dict[cla1.text].append(seeall_url.get_attribute('href'))\n",
    "\n",
    "        for th_url in third_url_elem:\n",
    "            if th_url.get_attribute('href') not in third_url_list:\n",
    "                third_url_list.append(th_url.get_attribute('href'))\n",
    "\n",
    "\n",
    "def make_classification():\n",
    "    \n",
    "    \"\"\"cla1_cla2_dict.csv 생성\n",
    "    cla1, cla2는 분류기준이며 cla2는 cla1에 속해있다\"\"\"\n",
    "    \n",
    "    for cla1, urls in cla1_seeall_url_dict.items():\n",
    "        for url in urls:\n",
    "            driver.get(url)\n",
    "            cla2_elem = driver.find_elements(By.CLASS_NAME, 'card-title')\n",
    "\n",
    "            for cla2 in cla2_elem:\n",
    "                if cla2.text not in cla1_cla2_dict[cla1]:\n",
    "                    cla1_cla2_dict[cla1].append(cla2.text)\n",
    "                    \n",
    "    for url in see_all_url:\n",
    "        driver.get(url)\n",
    "        third_url_elem2 = driver.find_elements(By.CLASS_NAME,'card > a')\n",
    "\n",
    "    for th_url in third_url_elem2:\n",
    "        if th_url.get_attribute('href') not in third_url_list:\n",
    "            third_url_list.append(th_url.get_attribute('href'))\n",
    "            \n",
    "    cla1_cla2_df = pd.DataFrame.from_dict(cla1_cla2_dict, orient='index')\n",
    "    cla1_cla2_df.to_csv('cla1_cla2_dict.csv')\n",
    "            \n",
    "    third_url_df = pd.DataFrame(third_url_list, columns=['third_url'])\n",
    "    third_url_df.to_csv('third_url.csv')\n",
    "\n",
    "\n",
    "def collect_title_url():\n",
    "    \n",
    "    \"\"\"논문의 title, url 수집, title_url.csv, cla2_title_dict.csv 생성\"\"\"\n",
    "    \n",
    "    title_url_df = pd.DataFrame(columns=['title', 'url'])\n",
    "    title_url_df.to_csv('title_url.csv')\n",
    "    title_url_csv = pd.read_csv('title_url.csv')\n",
    "\n",
    "    title_list = []\n",
    "    url_list = []\n",
    "    pdf_list = []\n",
    "    classification1 = []\n",
    "    classification2 = []\n",
    "    title_url_dict = {}\n",
    "    cla_dict=collections.defaultdict(list)\n",
    "    pdf_cla_dict={}\n",
    "\n",
    "    for url in third_url_list: \n",
    "        driver.get(url)\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\") \n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\") \n",
    "            time.sleep(3)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height       \n",
    "\n",
    "        title_url_elem = driver.find_elements(By.CLASS_NAME, 'col-lg-9.item-content > h1 > a')\n",
    "        classification2_elem=driver.find_elements(By.CLASS_NAME, 'artefact-header > h1')\n",
    "\n",
    "        for cla2 in classification2_elem:\n",
    "            classification2.append(cla2.text)            \n",
    "\n",
    "            for title_url in title_url_elem:\n",
    "                if title_url.get_attribute('href') not in title_url_csv['url']:\n",
    "\n",
    "                    title_list.append(title_url.text)\n",
    "                    url_list.append(title_url.get_attribute('href'))\n",
    "\n",
    "\n",
    "                    title_url_dict[title_url.text] = title_url.get_attribute('href')\n",
    "                    cla_dict[cla2.text].append(title_url.text)\n",
    "                    \n",
    "    title_url_df = pd.DataFrame.from_dict(title_url_dict, orient='index')\n",
    "    title_url_df.to_csv('title_url.csv')\n",
    "    cla_dict_df = pd.DataFrame(columns=['cla2', 'title'])\n",
    "    cla_dict_df = pd.DataFrame.from_dict(cla_dict, orient='index')\n",
    "    cla_dict_df.to_csv('cla2_title_dict.csv')\n",
    "\n",
    "\n",
    "def get_pdf_url():\n",
    "    \n",
    "    \"\"\"pdf주소 수집, title_cla_df.csv, title_pdf_df.csv 생성\"\"\"\n",
    "\n",
    "    title_url_csv = pd.read_csv('title_url.csv')\n",
    "    title_url_csv.rename(columns = {'Unnamed: 0':'title', '0':'url'}, inplace=True )\n",
    "    url_df = title_url_csv['url']\n",
    "\n",
    "    title_pdf_dict = collections.defaultdict(list)\n",
    "    title_cla2_dict = collections.defaultdict(list)\n",
    "    pdf_list = []\n",
    "\n",
    "    for url in url_df:\n",
    "        driver.get(url)\n",
    "\n",
    "        classify_url_elem = driver.find_elements(By.CLASS_NAME, 'col-md-12 > a')\n",
    "        title_elem = driver.find_elements(By.CLASS_NAME, 'col-md-12 > h1')\n",
    "        pdf_url = driver.find_elements(By.CLASS_NAME,'badge.badge-light')[0].get_attribute('href')\n",
    "\n",
    "        pdf_list.append(pdf_url)\n",
    "\n",
    "        for classify_url in classify_url_elem:\n",
    "            classify = classify_url.get_attribute('href')        \n",
    "\n",
    "            #print(classify_url)\n",
    "            for title_rough in title_elem:\n",
    "                title = title_rough.text\n",
    "\n",
    "                classify_section = classify.split('/')[-1].replace('-',' ')\n",
    "                if 'paperswithcode.com/task' in classify and classify_section not in title_cla2_dict[title]:        \n",
    "                    title_cla2_dict[title].append(classify_section)\n",
    "\n",
    "                for k in pdf_url:\n",
    "                    title_pdf_dict[title] = pdf_url\n",
    "                    \n",
    "    pdf_df = pd.DataFrame(pdf_list, columns=['pdf'])\n",
    "    pdf_df.to_csv('pdf.csv')\n",
    "                    \n",
    "    title_cla2_df = pd.DataFrame.from_dict(title_cla2_dict, orient='index')\n",
    "    title_cla2_df.to_csv('title_cla_df.csv')\n",
    "\n",
    "    title_pdf_df = pd.DataFrame.from_dict(title_pdf_dict, orient='index')\n",
    "    title_pdf_df.to_csv('title_pdf_df.csv')\n",
    "\n",
    "def download_pdf(download_path):\n",
    "    \n",
    "    \"\"\"pdf 다운로드\"\"\"\n",
    "    \n",
    "    print(f'전체 파일 수: {len(pdfs)}')\n",
    "\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "    else:\n",
    "        print('directory exists')\n",
    "\n",
    "    for num, (url, file) in enumerate(zip(pdfs, pdf_file)):\n",
    "\n",
    "        if os.path.isfile(f'{download_path}/{file}'):\n",
    "            print(f'{num} : {file}은 이미 존재하는 파일입니다.')\n",
    "        else:\n",
    "            print(f'{num} : {file} downloading')\n",
    "            os.system(f'curl \"{url}\" --output \"{download_path}/{file}\"')\n",
    "\n",
    "#pdf text변환: https://koreapy.tistory.com/829\n",
    "def pdf_to_txt(pdf_file):\n",
    "    \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "   \n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(pdf_file, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    \n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,\n",
    "                                 caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    \n",
    "    text = retstr.getvalue()\n",
    "    \n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    pdf_txt=[]\n",
    "    pdf_txt.append(text)\n",
    "    return text\n",
    "\n",
    "#오류제외 파일변환\n",
    "\n",
    "def pdf_to_txt_with_exception(pdf_file, download_path):\n",
    "    \n",
    "    \"\"\"pdf를 txt로 변환, 오류처리\"\"\"\n",
    "    \n",
    "    if not os.path.exists(f'{download_path}/txt'):\n",
    "        os.makedirs(f'{download_path}/txt')\n",
    "    else:\n",
    "        print('directory exists')\n",
    "        \n",
    "    for pdf in tqdm(pdf_file):  \n",
    "        txt_file_name = '.'.join(pdf.split('.')[:-1])\n",
    "        if os.path.isfile( f'{download_path}/txt/{txt_file_name}.txt' ) != True:\n",
    "            try:\n",
    "                text = pdf_to_txt(f'{download_path}/{pdf}')            \n",
    "                with open( f'{download_path}/txt/{txt_file_name}.txt', 'w', encoding='utf-8' ) as f:\n",
    "                    f.write(text)\n",
    "            except Exception as e:\n",
    "                print(e, pdf)\n",
    "\n",
    "def word_token(txt_path):\n",
    "    \n",
    "    \"\"\"단어 토큰화, keywords.csv생성\"\"\"\n",
    "    \n",
    "    def get_wordnet_pos(tagged_pos):\n",
    "        for pos in ['V', 'N', 'J', 'R']:\n",
    "            if tagged_pos.startswith(pos):\n",
    "                return pos.lower() if pos != 'J' else 'a'\n",
    "        return None\n",
    "\n",
    "    lemm = WordNetLemmatizer()\n",
    "    word_txt={}\n",
    "\n",
    "    for txt_file in tqdm(txt_path):\n",
    "        with open(txt_file, encoding='utf-8') as f:\n",
    "            txt = f.read().lower().replace('-\\n', '')\n",
    "\n",
    "\n",
    "        # 토크나이저\n",
    "            re_tokenizer = RegexpTokenizer('[a-zA-Z]{2,}')\n",
    "\n",
    "            # 토큰화\n",
    "            word_tokens = re_tokenizer.tokenize(txt)\n",
    "\n",
    "            # 불용어(stopwords) 제거\n",
    "            stop_words = stopwords.words('english')\n",
    "            stop_words.append('cid')\n",
    "            word_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "            #stemmer = PorterStemmer()\n",
    "            stemmer = SnowballStemmer('english')\n",
    "            word_tokens = [stemmer.stem(w) for w in word_tokens]\n",
    "\n",
    "            word_tokens = pos_tag(word_tokens)\n",
    "            word_tokens = [(w, get_wordnet_pos(tag)) for w, tag in word_tokens if get_wordnet_pos(tag) != None]\n",
    "            word_tokens = [lemm.lemmatize(word, pos=tag) for word, tag in word_tokens]\n",
    "\n",
    "            if word_txt.values() == None:\n",
    "                for f in FreqDist(word_tokens):\n",
    "                    word_txt[f] = txt_file\n",
    "                    \n",
    "    freq_df = pd.DataFrame.from_dict(FreqDist(word_tokens),  orient='index')\n",
    "#     freq_df.to_csv('keywords.csv')\n",
    "#     freq_csv = pd.read_csv('keywords.csv')\n",
    "    freq_df.rename(columns={'Unnamed: 0': 'keywords', '0': 'frequency'}, inplace=True)\n",
    "    freq_df.to_csv('keywords.csv')\n",
    "\n",
    "def google_search():\n",
    "    \n",
    "    \"\"\"keywords 구글로 뜻 찾기\"\"\"\n",
    "    \n",
    "#     keywords_copy = pd.read_csv('keywords.csv')\n",
    "#     keywords_copy.rename(columns={'Unnamed: 0': 'keywords', '0': 'frequency'}, inplace=True)\n",
    "    keywords = keywords_copy['keywords']\n",
    "    terminologies = {}\n",
    "    explains = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        driver.get('https://www.google.com')\n",
    "        search_elem = driver.find_element(By.CLASS_NAME, 'gLFyf.gsfi')\n",
    "        search_elem.clear()\n",
    "        search_elem.send_keys( f'what is {keyword} of deep learning' )\n",
    "        driver.find_element(By.CLASS_NAME, 'gNO89b').submit()\n",
    "\n",
    "        relatedQ_elem = driver.find_elements(By.CLASS_NAME, 'Wt5Tfe') \n",
    "        one_relatedQ_elem = driver.find_elements(By.CLASS_NAME, 'wQiwMc.ygGdYd.related-question-pair')    \n",
    "        explain_snippet_elem = driver.find_elements(By.CLASS_NAME,'LGOjhe')    \n",
    "        explain_check_elem = driver.find_elements(By.CLASS_NAME, 'yp1CPe.wDYxhc.NFQFxe.viOShc.LKPcQc > div > div > div > div')   \n",
    "        full_keyword_elem = driver.find_elements(By.CLASS_NAME, 'gL9Hy')\n",
    "\n",
    "\n",
    "        for full_keyword in full_keyword_elem:\n",
    "            if keyword in full_keyword.text:\n",
    "                keywords.replace(keyword, full_keyword.text.split(' ')[2], inplace=True)         \n",
    "\n",
    "        count = 0\n",
    "        try:\n",
    "            for click_number, questions in enumerate(one_relatedQ_elem):\n",
    "                for rQ in relatedQ_elem:\n",
    "                    if rQ.is_displayed:\n",
    "                        if count == 0:\n",
    "                            count += 1\n",
    "                            questions.click()\n",
    "                            \n",
    "                    for explain in explain_snippet_elem:\n",
    "                        explains.append(explain.text)\n",
    "                        if terminologies.get(keyword) == None:\n",
    "                            terminologies[keyword] = explain.text\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "\n",
    "        for explain in explain_check_elem:\n",
    "            if explain.is_displayed():            \n",
    "                explains.append(explain.text)\n",
    "                if terminologies.get(keyword) == None:\n",
    "                    time.sleep(1)\n",
    "                    terminologies[keyword] = explain.text\n",
    "\n",
    "        time.sleep(2)        \n",
    "        \n",
    "    word_explain = pd.DataFrame( columns=['keywords', 'explains'] )\n",
    "    word_explain['keywords'] = terminologies.keys()\n",
    "    word_explain['explains'] = terminologies.values()\n",
    "    word_explain.to_csv('word_explains.csv')\n",
    "\n",
    "    keywords_copy.to_csv('keywords_change.csv')\n",
    "\n",
    "def explain_translation():\n",
    "    Ttranslated = {}\n",
    "    translate_result = []\n",
    "\n",
    "    explain_translation = {}\n",
    "    keywords_translation = {}\n",
    "    word_explain_csv = pd.read_csv('word_explains.csv', index_col=0)\n",
    "\n",
    "    for explain in word_explain_csv['explains']:\n",
    "        translator = Translator()\n",
    "        translated_explanation = translator.translate(explain, src='en', dest='ko')\n",
    "        translate_result.append(translated_explanation.text)\n",
    "        explain_translation[explain] = translated_explanation.text #원문-번역 dict\n",
    "\n",
    "    explain_translation_df = pd.DataFrame( columns=['explains', 'translations'] )\n",
    "    explain_translation_df['explains'] = explain_translation.keys()\n",
    "    explain_translation_df['translations'] = explain_translation.values()\n",
    "    explain_translation_df.to_csv('explain_translation.csv')\n",
    "\n",
    "    keyword_explain_trans_df = word_explain_csv.merge(explain_translation_df, how='left', on='explains')\n",
    "    keyword_explain_trans_frq_df = keyword_explain_trans_df.merge(keywords_df, how='left', on='keywords')\n",
    "    keyword_explain_trans_frq_df = keyword_explain_trans_frq_df[['keywords','translations','frequency','explain']]\n",
    "    keyword_explain_trans_frq_df.to_csv('keyword_explain_trans_frq.csv')\n",
    "\n",
    "def order_by_frequency():\n",
    "\n",
    "\n",
    "    result = {}\n",
    "    sorted_result = {}\n",
    "    # keywords=keywords_all['단어']\n",
    "    replace_txt_name=[]\n",
    "\n",
    "    keywords_csv = pd.read_csv('keywords.csv', index_col=0)\n",
    "    keywords = keywords_copy['keywords']\n",
    "\n",
    "    for keyword in keywords:\n",
    "        result[keyword] = {}\n",
    "\n",
    "        for txt_file in glob('./Downloads/pdf_download/txt/*.txt'):\n",
    "            txt_file_name = os.path.basename(txt_file).replace('.txt','')        \n",
    "            with open(txt_file, encoding='utf-8') as file:\n",
    "                sentences = file.read().lower().replace('-\\n', '').replace('\\n', ' ').split('.')\n",
    "            sentences = ' '.join(sentences)\n",
    "            count = sentences.count(keyword)        \n",
    "\n",
    "            result[keyword][txt_file_name] = count              \n",
    "\n",
    "            \"\"\"이거는 txt name으로 pdf가 몇 번 등장했는지 알려줌\n",
    "            여기에다가 \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            sorting = sorted(result[keyword], key= lambda x : result[keyword][x], reverse=True)[:5]\n",
    "            sorted_result[keyword] = sorting\n",
    "\n",
    "        \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": \"C:/Users/kimEn/anaconda3/envs/deep_learning_dictionary\", #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "})\n",
    "driver = webdriver.Chrome('chromedriver.exe', options=options)\n",
    "\n",
    "\n",
    "\n",
    "class collect_informations():\n",
    "    first_collect_url()\n",
    "    make_classification()\n",
    "    collect_title_url()\n",
    "    get_pdf_url()\n",
    "    \n",
    "pdf_to_txt_with_exception(pdf_file, download_path)\n",
    "txt_path = glob('C:/Users/kimEn/Downloads/pdf_download/txt/*.txt')\n",
    "word_token(txt_path)\n",
    "chrome_option()\n",
    "google_search()\n",
    "explain_translation()\n",
    "\n",
    "third_url_csv = pd.read_csv('third_url.csv')\n",
    "cla1_cla2_df = pd.read_csv('cla1_cla2_dict.csv', index_col=0)\n",
    "cla1_cla2_df.T\n",
    "\n",
    "\n",
    "pdf_csv = pd.read_csv('title_pdf_df')\n",
    "pdf_csv\n",
    "\n",
    "pdf_csv = pd.read_csv('pdf.csv', index_col=0)\n",
    "pdfs = pdf_csv['pdf'].values.tolist()\n",
    "pdf_file = [ pdf.split('/')[-1] for pdf in pdf_csv['pdf'].values if 'arxiv' in pdf ]\n",
    "    \n",
    "    \n",
    "    \n",
    "download_path = f'C:/Users/kimEn/Downloads/pdf_download'\n",
    "download_pdf(download_path)\n",
    "\n",
    "last_last = pd.merge(left=keywords_all, right=classification_last, how='left', on='단어')\n",
    "# last_result = last_last.to_html('결과.html', encoding=\"utf-8-sig\")\n",
    "\n",
    "keywords_csv = pd.read_csv('keywords.csv')\n",
    "keywords_csv.rename(columns={'Unnamed: 0': 'keywords', '0': 'frequency'}, inplace=True)\n",
    "keywords_csv\n",
    "\n",
    "keywords_copy = keywords_csv.copy()\n",
    "keywords_copy\n",
    "keywords_copy['keywords']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
